Notes

Working on setting up the PiZero, decided to disable bluetooth, hciart and made it headless through the setup. Currently awaiting the Pi0 to perform it's update and upgrade...why did a choose one of the slowest pi's of all...

Next the plan is to perform the temporary setup of the Canon EOS-6D, capture some images just to make sure that loop is all working..
It occurs to me that my monitor being curved could become an issue and I might have to use a different one

agghhh...0% [2 base-files 56.1 kB/71.1 kB 79%] [Waiting for headers] [1 raspi-firmware 312 kB/12.8 MB 2%]   5,287 B/s 16h 15min 6s   
worse than that it seems stuck...it was but I was able to reboot the PI from the keyboard.... Ok turns out the Wan connection dropped for unknown reasons

I had to reboot and it eventually came back up... something worth looking out for in the future, might have to switch to my other wan.

...going to proceed with the update again... seems to be going smoother this time. Turns out the usb hub that i am using with the pi zero allows external power
seems like it could be why the wan dropped so im gonna dig around where I found this pi zero and see if i can find the tiny tiny barrel connector for the usb power
otherwise it will likely become an issue when it comes time to capture images. I am going in completely headless so all i need is the camera and wifi dongle
while these updates are taking place is a perfect time to go looking...

found one that fits hopefully its the correct voltage, as soon as I touched the hub the pi rebooted? So will have to start the update yet again, several things did finish through

ok to save time i will run this update later, like while im asleep and not waiting on doing the next step...so lets try and capture some images from a dlsr


maybe the curved screen will make the images more like they were collected from a fisheye lens, it could add some realism?

...ok these packages are taking forever so I will now build the rover in KSP if I cant find something prebuilt I can use then takeoff and head to the Mun..

well that was short lived, the updates got held up so no zmq until i do complete the system update. So I guess I have plenty of time to get to the mun. 

------after dealing with significant issues trying to install zmq on the pizero i have instead opted to startover on a RPi 2 Model B.
I mistakenly thought I had a new Pizero but it is the original, though the Pi wont be doing much having a single core processor will simply not work
or prove more exedingly difficult than I care for this simple task, so we are goint with a 4 core 900mz pi2B
hate starting over after such a struggle but it should be quicker to get back were I was at anyway. 

good thing I kept good notes on commands i've run :D 
seems I was trying to make this way harder than it should be, even the SD card was significantly faster than in the zero


wow that took like 30 mins to get back where I left off no dependecy issues, off to a great start now
I also discovered I have an old old saved game that already has a lunar rover on the Mun, its not very pretty
and I think the wheels arent all moving in the correct direction (its pretty janky) but it might work well for the initial testing at least.

here we are 8 hours into the project and i've just taken my first picture with the RPI, i will upload it to the repo.
ok i've taken 3 pictures, and I think I have that part ironed out just fine. Now comes the real stuff..

I can use these first 3 images, well specifically image 3 to do some object detection. Down the rabbit hole I go.

ok I took a break and thought about what im trying to accomplish here and decided after some research that I need to develop a good 
set of training data, 1 good image of course isnt going to be enough so I've written some scripts that will capture images in a much quicker fashion
i will control the rover myself and capture a bunch of images then train on that data so the final product will hopefully know what its looking at.

ok I've captured 430 images as the rover progresses the surface of mun, reviewing some of the images I hope I dont need to retake them, some are fairly blurry
I suspect this is a camera setting. I will begin training with this data tomorrow, it's almost 1 am and I need to call it a day.

Ok I've began processing all the 430 images, there are a number of steps that I need to take before I start using these images for training.
I've split each of the 430 high res images into 512x512 size squares 88 per image. Now I am working on a sorting algorithm so I can trim down the 37,928 files into categories 
this will be done based on light and dark weights, color content to detect the rover etc... it would take a long time to do by hand and find the good data so i'll do it in python
it's over 6.4GB of image data. The images themselves are only 816mb. I will zip these before uploading to Github, and probably remove them later when they are no longer needed.
good thing I have plenty of HD space :)

ok first pass of sorting them by content a little resulted in:
     17,000 images that contain empty soil (not even close)
     6,000 images that contain horizon (closer but unlikely)
     1,859 images that contain rocks or the rover (not even close)
     12,000 images that contain space (not even close either)
     26 images that contain the sun (close but i still expect a lot more)

     So I will break these down further by hand...maybe it wont take too long...::fingers crossed::
     adding categories for rover by itself and anomaly for in game things on the UI

looks like some of the images have a black bar on the right side, so I'll make a script to remove those
...after sorting for a bit, im gonna go back and try and resort with better accuracy... turns out it takes a while to go through 6 gigs of pictures even with a big monitor

split and cluster worked a little better, but I'm still going to try some other methods of clustering.
Turns out this is a pretty tough problem, there isnt much difference in the shadow of a rock and the darkness of space...
so im gonna try some torch deep clustering now.

the clustering here is good, but I just dont feel like taking the time to sort through 37000 images so I am gonna go with 25 start full images
then im gonna run it through the same process and manually sort and label the results.

ok i have a much smaller set of images (~2000) that I dont mind sorting by hand a bit more, then I will write a script to duplicate the images by mirroring them, perhaps also darken them slightly for another duplicate
here are my categories
    anomaly: anything in game that isnt in virtual environment, eg. mouse pointers and in game windows etc..
    close_rocks
    distant_rocks
    empty_terrain
    horizon
    night
    rock
    rover
    space
    sun

Ok i've decided to also pump up the numbers by duplicating the images and making them walk +/-20% darkness/lightness
This should result in about 80K images which is a pretty decent data set, some classes are much better covered than others but lets see what happens.
I suspect I will see rocks everywhere, since in my sample images rocks are everywhere...
I also wrote a script to rotate the image +/- 5deg, should have ample training material

2037 files to start.

$ python fliq_east_west.py
Processing anomaly...
Flipped 70 images in anomaly
Processing close_rocks...
Flipped 560 images in close_rocks
Processing distant_rocks...
Flipped 172 images in distant_rocks
Processing empty_terrain...
Flipped 1302 images in empty_terrain
Processing horizon...
Flipped 130 images in horizon
Processing night...
Flipped 954 images in night
Processing rock...
Flipped 602 images in rock
Processing rover...
Flipped 84 images in rover
Processing space...
Flipped 132 images in space
Processing sun...
Flipped 68 images in sun
East-West flipping complete!

4074

$ python rotate_images.py
Processing anomaly...
Rotated 840 images in anomaly
Processing close_rocks...
Rotated 6720 images in close_rocks
Processing distant_rocks...
Rotated 2064 images in distant_rocks
Processing empty_terrain...
Rotated 15624 images in empty_terrain
Processing horizon...
Rotated 1560 images in horizon
Processing night...
Rotated 11448 images in night
Processing rock...
Rotated 7224 images in rock
Processing rover...
Rotated 1008 images in rover
Processing space...
Rotated 1584 images in space
Processing sun...
Rotated 816 images in sun
Rotation complete!

40888 images 4.62GB...  
It took a few minutes and I now realize i need to move to multi threading before I run the next revision, it will still be slowest
So I have a new version of the duplicate and vari brightness script that uses multiprocessing pools

NOTE: next time add timestamping. 

$ python brightness_varations_parallel.py
Using 32 CPU cores
Processing anomaly...
Generated brightness variations for 35280 images in anomaly
Processing close_rocks...
Generated brightness variations for 282240 images in close_rocks
Processing distant_rocks...
Generated brightness variations for 86688 images in distant_rocks
Processing empty_terrain...
Generated brightness variations for 656208 images in empty_terrain
Processing horizon...
Generated brightness variations for 65520 images in horizon
Processing night...
Generated brightness variations for 480816 images in night
Processing rock...
Generated brightness variations for 303408 images in rock
Processing rover...
Generated brightness variations for 42336 images in rover
Processing space...
Generated brightness variations for 66528 images in space
Processing sun...
Generated brightness variations for 34272 images in sun
Brightness variations complete! Total images: ~1,760,000

2.05 Million images 213GB... I might take this back one step and only do the 5 degree rotation...instead I'll just train on a subsample for now and run the full training overnight.

$ python subsample_for_test_data.py
Total images: 2053296
anomaly: 1374 train, 344 test
close_rocks: 10996 train, 2749 test
distant_rocks: 3376 train, 845 test
empty_terrain: 25566 train, 6392 test
horizon: 2552 train, 638 test
night: 18732 train, 4684 test
rock: 11820 train, 2956 test
rover: 1648 train, 413 test
space: 2592 train, 648 test
sun: 1335 train, 334 test
Subsampled to ~100000 images!

now the training can begin!

Something magical about that GPU fan spinning up as it reaches 100% utilization. Only using 5GB of it's 40
INCREDIBLE!
$ python train_and_save.py
Epoch 1/5
train Loss: 0.2782 Acc: 0.9098
test Loss: 0.0294 Acc: 0.9951
Epoch 2/5
train Loss: 0.0242 Acc: 0.9956
test Loss: 0.0053 Acc: 0.9993
Epoch 3/5
train Loss: 0.0095 Acc: 0.9984
test Loss: 0.0025 Acc: 0.9995
Epoch 4/5
train Loss: 0.0051 Acc: 0.9993
test Loss: 0.0017 Acc: 0.9997
Epoch 5/5
train Loss: 0.0037 Acc: 0.9994
test Loss: 0.0017 Acc: 0.9994
Training complete in 8m 32s
Model saved as rock_classifier.pth

Amazing results! I must however remain skeptical until I run some test stills back through and see using bounding boxes if its working.

Im at a crossroad now, there are some errors in the detection boxes, but overall they are kind of correct, 
I could use some logic in how the detections are processed, or just remove the close/near/far rocks and just call them all rocks. 
Will have to do some tinkering...
    once I figure this out I think the control portion will be very very easy in comparison.

Estimated time in so far is probably about 15-18 hours, most on sunday 
    If i dont count coming through the first 40k images before figuring out how to reduce them and still get good data maybe 12 hours.

Ok after I ran through severl versions of detect and display found filter boxes, and tweaked setting to get some fairly decent results
I decided it would probably be worthwile to create a "full" dataset using the 2million image variations that I produced previously
I can let this run overnight and then try the results using this training data in the morning ideally I will get better results.
Not that the results i've gotten so far arent impressive and great display of this technology more data is most always better so lets see what happens.
I can always not use this one later. I still have my most recent training set anyway. might as well put those cpu/gpu cycles to good use.


$ python split_full_dataset.py
anomaly: 28224 train, 7056 test
empty_terrain: 524966 train, 131242 test
horizon: 52416 train, 13104 test
night: 384652 train, 96164 test
rock_combined: 537868 train, 134468 test
rover: 33868 train, 8468 test
space: 53222 train, 13306 test
sun: 27417 train, 6855 test
Dataset split complete!

performing overnight training
$ python train_rock_classifier_full_dataset.py
Classes: ['anomaly', 'empty_terrain', 'horizon', 'night', 'rock_combined', 'rover', 'space', 'sun'], Train: 1642633, Test: 410663
Epoch 1/10
train Loss: 0.0108 Acc: 0.9964
test Loss: 0.0007 Acc: 0.9995
Epoch 2/10
train Loss: 0.0009 Acc: 0.9995
test Loss: 0.0008 Acc: 0.9995
Epoch 3/10
train Loss: 0.0009 Acc: 0.9995
test Loss: 0.0007 Acc: 0.9995
Epoch 4/10
train Loss: 0.0008 Acc: 0.9995
test Loss: 0.0007 Acc: 0.9995
Epoch 5/10
train Loss: 0.0008 Acc: 0.9995
test Loss: 0.0007 Acc: 0.9995
Epoch 6/10
train Loss: 0.0008 Acc: 0.9995
test Loss: 0.0007 Acc: 0.9995
Epoch 7/10
train Loss: 0.0007 Acc: 0.9995
test Loss: 0.0007 Acc: 0.9995
Epoch 8/10
train Loss: 0.0007 Acc: 0.9995
test Loss: 0.0007 Acc: 0.9995
Epoch 9/10
train Loss: 0.0007 Acc: 0.9995
test Loss: 0.0007 Acc: 0.9995
Epoch 10/10
train Loss: 0.0007 Acc: 0.9995
test Loss: 0.0007 Acc: 0.9995
Training complete in 466m 41s
Model saved as rock_classifier_full.pth

Ok after fighting GIT because I checked in a 2 gig training zip file 4 commit's ago. I realized I need to make some decisions on how to best proceed with the automation controls
After doing some critical thinking I've decided the best course of action is to create a test controller that runs on the RPI, I will use c++ just to throw a subtle curve into the project 
    especially since i'm trying to create some real world examples of solving problems since all my 14 years of other experience is all classifed or propritary 

So I will make a command line driven rover controller, where command args will perform functions, eg. ./rovercontrol -execute turn 30 -speed 10  
This is the next step that serves two functions, control and next proof of concept. Afterward I can then create a logic engine based on the image detector
    I can call this controller using system calls, it just needs to fire off the command over tcp to the BaseStation pc to perform the commands, I dont think there is any required ack, but I could be wrong
    Ok I have te rover controller next I have decided to make a python Control Caller app, I have a first version of this app but ..,
    I want to implement a command line menu so I can do several things to test out functionality

next I've decided for the automated controller I want to use the programming language GO, I havent made a GO application before but I hear good things and since it helps fulfil job requirment
    and also I believe it is uniquely tailored to state machines this seems like a great time to go forward with a GO app. I'm not sure what it takes to build go but I will find out in the next few hours...
